# Evaluation {#ch:evaluation}

The nice properties of the Rasch model (c.f. Section
\@ref(sec:whyrasch)) only hold when the data and model agree. It is
therefore important to study and remove discrepancies between model
and data. This chapter explains several techniques that aid in the
evualation of model fit.

* Item fit (\@ref(sec:itemfit))
* Person fit (\@ref(sec:personfit))
* Differential item functioning (\@ref(sec:dif))
* Item information (\@ref(sec:iteminformation))
* Reliability (\@ref(sec:reliability))

These topics adress different aspects of the solution. In practice, we
have found that item fit is the most important concern.

## Item fit {#sec:itemfit}

The philosophy of the Rasch model is different from conventional
statistical modeling. It is not the task of the Rasch model to account
for the data. Rather it is the task of the data to fit the Rasch
model. We saw this distinction before in Section
\@ref(sec:adaptmodel).

The goal of model fit assessment is to explore and quantify how well
empirical data meet the requirements of the Rasch model. One way to
gauge model fit is to compare the observed probability of passing an
item to the fitted item response curve for endorsing the item.

The fitted item response curve for each item $i$ is modeled as: 

$$P_{ni}= \frac{exp( \beta_{n} - \delta_{i})}{1+exp(\beta_{n}-\delta_{i})}, $$ 

where $\beta_n$ is the ability of person $n$ and $\delta_i$ is the
difficulty of item $i$. It is thus natural to compare the observed and
modelled probabilities at various levels of ability.

### Well-fitting item response curves

```{r smoccdata3, echo = FALSE, warning = FALSE}
library(gseddata)
library(dmetric)
data <- get_data(cohorts = 53)
items <- get_itemnames(data)
```

```{r smoccfit3, echo = FALSE, warning = FALSE}
varlist <- list(adm = c("cohort", "subjid", "agedays"), 
                items = items,
                cov = NULL)
model <- fit_dmodel(varlist, data, name = "57_0")
```

```{r ploticc, echo = FALSE, results = 'hide', fig.keep = 'all', fig.height = 7, warning = FALSE, fig.cap = '(ref:ploticc)'}
theme_set(theme_light())
p1 <- plot_p_d_item(data = data, model = model, items = "ddifmd005")[[1]]
p2 <- plot_p_d_item(data = data, model = model, items = "ddigmm060")[[1]]
gridExtra::grid.arrange(p1, p2)
```

(ref:ploticc) Empirical and fitted item response curves for two
milestones from the DDI (SMOCC data).

Figure \@ref(fig:ploticc) shows the item characteristics curves of two
DDI milestones. The orange line represents the empirical probability
at different ability levels. The dashed line represents the estimated
item response curve according to the Rasch model. The empirical and
estimated curves are close together, so both items fit the model very
well.

### Item response curves showing severe underfit

These examples of item response curves fit the model nicely. There are
however many cases where things are less bright. 

```{r smoccfit4, echo = FALSE, warning = FALSE}
set.seed(12345)
plus_items <- paste0("hypgmd00", 1:5)
varlist <- list(adm = c("cohort", "subjid", "agedays"), 
                items = c(items, plus_items),
                cov = NULL)
data2 <- as.data.frame(data)
data2 <- data2 %>% 
  mutate(hypgmd001 = as.integer(runif(n = nrow(data2), 0, 2)), # flat
         rand   = runif(nrow(data2), 0 , 1),
         hypgmd002 = ifelse(agedays  > 65 & rand > 0.2, 1, 0), # flat2
         hypgmd002 = ifelse(agedays <= 65 & rand < 0.2, 1, hypgmd002), # flat2
         hypgmd003 = ifelse(agedays > 65, 0, 1), # gutman age
         hypgmd004 = ifelse(model$beta_l$b < 2, 0, 1), # gutman d
         hypgmd005 = ifelse(model$beta_l$b > 2 & runif(nrow(data2), 0, 1) > 0.1, 1, 0) # gutman d_error
         ) %>% 
  dplyr::select(-rand)
b_fixed <- model$fit$b
model2 <- fit_dmodel(varlist = varlist, data = data2, 
                     name = "57_0_plus", b_fixed = b_fixed)
```


```{r plothyp1, echo = FALSE, results = 'hide', fig.keep = 'all', fig.height = 11, warning = FALSE, fig.cap = '(ref:plothyp1)', message = FALSE}
theme_set(theme_light())
p1 <- plot_p_d_item(data = data2, model = model2, items = "hypgmd001")[[1]]
p2 <- plot_p_d_item(data = data2, model = model2, items = "hypgmd002")[[1]]
p3 <- plot_p_d_item(data = data2, model = model2, items = "hypgmd003")[[1]]
gridExtra::grid.arrange(p1, p2, p3)
```

(ref:plothyp1) Three simulated items that illustrate various forms of item misfit.

Figure \@ref(fig:plothyp1) shows three forms of severe underfit. The
probability to pass item `hypgmd001` is almost constant across
ability, so retaining this item essentially only adds to the noise.
Item `hypgmd002` converges to an asymptote around 80 percent and has a
severe dip in the middle. The dip is caused by the strong relation to
age. Item `hypgmd003` appears to have the wrong coding. In addition,
the spike-like behavior in the middle is often seen when two or more
different items are erroneously given the same code.

Removing of items like these could substantially improve model fit.

### Item response curves showing overfit

```{r plothyp2, echo = FALSE, results = 'hide', fig.keep = 'all', fig.height = 7, warning = FALSE, fig.cap = '(ref:plothyp2)', message = FALSE}
theme_set(theme_light())
p1 <- plot_p_d_item(data = data2, model = model2, items = "hypgmd004")[[1]]
p2 <- plot_p_d_item(data = data2, model = model2, items = "hypgmd005")[[1]]
gridExtra::grid.arrange(p1, p2)
```

(ref:plothyp2) Two simulated items that illustrate item overfit.

Figure \@ref(fig:plothyp2) shows two forms of overfit. The curve of item
`hypgmd004` is much steeper than the modelled curve. Thus, just this
one item is exceptionally well-suited to distinguish children with a
$D$-score below 50 from those with a $D$-score above 50. Note that the
item isn't sensitive anywhere else on the scale. In general, having
items like these is good news, because they allow us to increase the
reliability of the instrument. One should make certain though, that
FAIL and PASS scores are all measured (not imputed) values.

Multiple perfect items could hint to a violation of the *local
independence assumption* (c.f. Section \@ref(sec:measurementmodel)).
Developmental milestones sometimes have combinations of responses that
are impossible. For example, one cannot walk without being able to
stand, so the inconsistent combination (stand: FAIL, walk: PASS) will
not be observed. This leads to more consistent responses that would be
expected by chance alone. In principle, one could combine the two such
items into one three-category items, which effectively set the
probability of inconsistent combinations to zero.

Item `hypgmd005` is also steep, but has an asymptote around 80
percent. This causes discrepancies between the empirical and modelled
curves around the middle of the probability scale. In general, such
items could be removed if a sufficient number of better items are
available.

### Item infit and outfit

Item fit can quantified by item *infit* and *outfit*. Both are
aggregates of the model residuals. The observed response $x_{ni}$ of
person $n$ on item $i$ can be $0$ or $1$. The expected response
$P_{ni}$ is modelled as

$$P_{ni} = \frac{exp(\hat\beta_{n} - \hat\delta_{i})}{1 + exp(\hat\beta_{n} - \hat\delta_{i})}$$, 

where $\hat\beta_{n}$ is the estimated ability of person $n$, and
where $\hat\delta_{i}$ is the estimated difficulty of item $i$. The
standardized residual $z_{ni}$ is the difference between the observed
response $x_{ni}$ and the expected response $P_{ni}$, divided by the
expected binomial standard deviation:

$$z_{ni} = \frac{x_{ni}-P_{ni}}{\sqrt{W_{ni}}}$$, 

where the expected response variance $W_{ni}$ is calculated as 

$$W_{ni} = P_{ni}(1-P_{ni})$$. 

*Item infit* is the sum of the squared residuals divided by the sum of
the expected response variances $W_{ni}$.

$$\mathrm{Infit} = \frac{\sum_{n}^N (x_{ni}-P_{ni})^2}{\sum_n^N W_{ni}}$$

*Item outfit* is calculated as the average (over $N$ persons) of the
squared standardized residual

$$\mathrm{Outfit} = \frac{\sum_{n}^N z_{ni}^2}{N}$$

The expected value of both infit and outfit is equal to 1.0. The
interpretation is as follows:

* If infit and outfit are 1.0, then the item perfectly fit the Rasch
model, as in Figure \@ref(fig:ploticc);

* If infit and outfit > 1.0, then the item has underfit. The amount of
underfit is quantified by infit and outfit, as in \@ref(fig:plothyp1);

* If infit and outfit > 1.0, then the item fit the model better than
expected (overfit). The amount of overfit is quantified by infit and
outfit, as in \@ref(fig:plothyp2).

Infit is more sensitive to discrepancies in the middle of the
probability scale, whereas outfit is the better measure for
discrepancies at probabilities close to 0 or 1. Lack of fit is
generally easier to spot at the extremes. The two measures are highly
correlated. Infit is generally considered more important.

Values near 1.0 are desirable. There are no cut-and-dried [cutoff
values](https://www.rasch.org/rmt/rmt83b.htm) for infit and outfit. In
general, we want to remove underfitting items, e.g., item with infits
or outfits higher than, say, 1.3. Overfitting items with values lower
than 1.0 are not harmful. Preserving these items may actually help to
increase reliability of the scale. The cut-off chosen also depends on
the number of available items. When there are many items to choose
from, we could use a stricter criterion, say infit and outfit < 1.0 to
select only the absolutely best items.

### Infit and outfit in the DDI {#sec:fitddi}

```{r fitplot, echo = FALSE, fig.cap = '(ref:fitplot)', fig.height = 4}
oldpar <- par(mfrow = c(1, 2))
hist(model$item_lfit$infit, breaks = 20, col = "grey", 
     border = FALSE, xlim = c(0, 1.2), ylim = c(0, 12),
     main = "", xlab = "Item infit", ylab = "Number of milestones")
hist(model$item_lfit$outfit, breaks = 20, col = "grey", 
     border = FALSE, xlim = c(0, 1.2), ylim = c(0, 12), 
     main = "", xlab = "Item outfit", ylab = "Number of milestones")
par(oldpar)
```

(ref:fitplot) Frequency distribution of infit (left) and outfit
(right) of 57 milestones from the DDI (SMOCC data).

Figure \@ref(fig:fitplot) displays the histogram of the 57 milestones
from the DDI (c.f. Section \@ref(sec:ddi)). Most infits are within the
the range 0.6 - 1.1, thus indicating excellent fit. The two milestones
with extremely low infits are `ddigmd052` and `ddigmd053`. These two
items actually screen for paralysis for newborns, so the data contain
hardly any FAILs on these milestones. The outfit statistics also
indicate good fit.

## Person fit {#sec:personfit}

The person fit quantifies the extent to which the responses of a given
person conform to the Rasch model expectation. The Rasch model expects
that a more able person has a greater probabilty to pass an item than
a less able person. Fit analysis evaluates the extend to which this is
true. 

### Person infit and outfit

In parallel to item fit, we can calculate *person infit* and *person
outfit*. Both statistics evaluate how well the responses of the
persons are consistent with the model. The outfit statistic is heavily
influenced by outlying responses that do not fit the expected pattern.
Outfit is high, for example, when the person fails easy items but
passes difficult items. The infit is the information weighted fit and
is more sensitive to inlying, on-target, unexpected responses.

Similar to item fit, person fit is also calculated from the residuals,
but aggregated in a different way. Person infit is calculated as

$$\mathrm{Infit} = \frac{\sum_{n}^L (x_{ni}-P_{ni})^2}{\sum_n^L W_{ni}}$$

and person outfit is

$$\mathrm{Outfit} = \frac{\sum_{n}^L z_{ni}^2}{L}$$

As a threshold for person fit < 3.0 can be used to clean out persons
with inplausible response patterns.

### Person infit and outfit in the DDI

```{r personfitplot, echo = FALSE, fig.cap = '(ref:personfitplot)', fig.height = 4}
oldpar <- par(mfrow = c(1, 2))
hist(model$person_lfit$infit, breaks = c(seq(0, 1000, 0.25)), col = "grey", 
     border = FALSE, xlim = c(0, 5), freq = TRUE, ylim = c(0, 7000),
     main = "", xlab = "Person infit", ylab = "Number of measurements")
hist(model$person_lfit$outfit, breaks = c(seq(0, 1000, 0.25)), col = "grey", 
     border = FALSE, xlim = c(0, 5), freq = TRUE, ylim = c(0, 7000),
     main = "", xlab = "Person outfit", ylab = "Number of measurements")
par(oldpar)
```

(ref:personfitplot) Frequency distribution of person infit (left) and
person outfit (right) for 16538 measurements of the DDI (SMOCC data).

Figure \@ref(fig:personfitplot) displays the frequency distribution of
person infit and person outfit 16538 measurement of the DDI in the
SMOCC data. The large majority of the values falls below 3.0. For
infit, only 43 out of 16538 measurements (0.3 percent) is above 3.0.
For outfit, there are 446 out of 16538 measurement (2.7 percent) above
3.0. Expect the solution to improve after deleting these measurements.

## Differential item functioning (DIF) {#sec:dif}

An important aspect of item fit to the Rasch model is the assumption that items have the same difficulty in different subgroups of respondents, which is called Differential Item Functioning (DIF). Zumbo (1999) describes a clear definition: "DIF occurs when examinees from different groups show differing probabilities of success on (or endorsing) the item after matching on the underlying ability that the item is intended to measure." [@zumbo1999] The examination of DIF can be done by modelling the probability of endorsing an item in a logistic regression model by the ability from the Rasch model and a grouping variable. When the grouping variable significantly explains residual variance of the item scores after adjusting for the ability, the item presents DIF for that group. DIF can be visually inspected by plotting the curves for the subgroups separately. For example when inspecting the DIF for male and female respondents, the following two milestones do not shown DIF:

```{r dif1, echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=3,fig.show='hold'}

plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddigmd063")[[1]]
plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddifmd011")[[1]]
```

However, these two items do show a small amount of DIF. This means that the difficulty for the items for male respondent is different than for female respondents. The milestone "Says three words" is less difficult for female respondents, whereas the milestone "Walks while holding onto play-pen or furniture" is less difficult for the male respondents.

```{r dif2, echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=3,fig.show='hold'}
plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddicmm039")[[1]]
plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddigmm067")[[1]]
```

## Item information {#sec:iteminformation}

### Item information at a given ability

The item information is defined as the amount of psychometric information an item contains for a given ability. The item information can be plotted for each item. In the Figure below, the item information curves for some example milestones are displayed. The amount of information for an item is maximized around the item-difficulty parameter. 

```{r iic, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 3}
#item information curve
## bereken item info by difficulty
 info <- function(beta, delta) {(exp(beta-delta)/(1+exp(beta-delta)) * (1-exp(beta-delta)/(1+exp(beta-delta))))}
 delta <- model$fit$b["ddigmm060"]
 betar <- c(-10,2)
 beta = seq(betar[1], betar[2], length = 200)
 iteminfo1 <- data.frame(ability=beta,I=info(beta,delta))

 delta <- model$fit$b["ddicmm039"]
 betar <- c(-2,10)
 beta = seq(betar[1], betar[2], length = 200)
 iteminfo2 <- data.frame(ability=beta,I=info(beta,delta))


 iteminfo1$item <- "Rolls over back to front"
 iteminfo2$item <- "Says three words"
 df <- rbind(iteminfo1,iteminfo2)
 
 ggplot(df, aes(ability,I, group=item, color=item))+geom_line()+xlab("Ability")+ylab("Item information")

```

The formula to obtain the item information is the first derivative of the item response curve and can be written as follows: $$I(\delta)=P_i(\delta)(1-P_i(\delta))$$ where $P_i(\delta)$ is the conditional probability of endorsing an item. For example for milestone "Says three words" the $\delta_i$ equals $3.81$. The probability of endorsing this item for a person with an ability level of $2$ standard deviation above the mean is $$P_{ni}= \frac{exp(2 -  3.81)}{1+exp(2-3.81)}$$ $$P_{ni}= 0.14$$
So for an ability level of $2$ standard deviations above the mean $\beta_n=2$, milestone "Says three words" has an information value of $$I(\delta)=0.14(1-0.14)$$ $$I(\delta)=0.12$$ 


### Item information at a given age

The item information can also be expressed against age. By doing so, one can identify at what age an item provides the most information.  

```{r iia, echo=FALSE, message=FALSE,  warning=FALSE,  fig.height = 3}
colnames(model$dscore)[7] <-"b"
model$dscore$age <- model$dscore$agedays/12
#model in logit
refs<- calculate_references(model = model, metric="logit")
refs$month <- refs$age


#item information curve
## bereken item info by difficulty
 info_age <- function(betas, p = 50, reference) {
 pd <- matrix(betas, nrow = length(betas), ncol = length(p)) +
    matrix(1 * qlogis(p / 100),
           nrow = length(betas), ncol = length(p), byrow = TRUE)
  pa <- approx(x = reference$mu, y = reference$month, xout = as.vector(pd))$y
  pa <- matrix(pa, ncol = length(p))
  pda <- data.frame(round(pd, 2), round(pa, 2))
  names(pda) <- c("delta", paste0("A", p))
  pda
 }
  

 iteminfo1$age <-info_age(betas = iteminfo1$ability, reference = refs)$A50
 iteminfo2$age <-info_age(betas = iteminfo2$ability, reference = refs)$A50

 df <- rbind(iteminfo1,iteminfo2)
 
 ggplot(df, aes(age,I, group=item, color=item))+geom_line()+xlab("Age(months)")+ylab("Item information")

```

The item information at a given age, can ultimately help determine at what age the item can best be administered. The item will be most informative, when adiminered at the age at which 50% of the respondents will pass the item. Administering the item at this age can be helpful when the item is used in an instrument to determine the ability of a population. However, when the item is used in a screening instrument, the goal would be to identify respondents that underperform compared to a norm group. In that case, the item can best be asessed at the age where 90% of respondents will pass the item. 

## Reliability {#sec:reliability}

The *person seperation index* is a measure for reliability for the Rasch model that is comparable to Cronbach's $\alpha$. In that context we define the reliability as the proportion of variance of the estimated person estimates (i.e. abilities) and the total variance including error. PSI obtained as follows: $PSI = \frac{\sigma_{\hat\beta}^2 -\hat\sigma_{\hat e}^2}{\hat\sigma_{\hat\beta}^2}$, where $\sigma_{\hat\beta}^2$ is the variance of the estimated abilities and $\sigma_{\hat e}^2$ is the error variance of the abilities. The standard error of measurement can be obtained from this information as $SEM = \sqrt{\sigma_{\hat e}^2}$, which reflects the precision of the ability estimation. Below the $PSI$ and $SEM$ are calculated for our example data. 

---
# Ik weet niet zeker of het nuttig is om PSI te bespreken, maar het blijkt wel dat veel onderzoekers een reliability maat fijn vinden om te hebben. Kun jij hieronder kijken of mijn berekening klopt? Ik heb de functie een beetje geannoteerd.
#Het idee is om aan de hand van de model resultaten, item data te simuleren en deze data weer te gebruiken om een ability score te schatten. De originele ability uit het mdoel is dan de true en de geschatte uit de simulatie de etsimated. Daarmee ga ik dan de PSI berekenen. Ik vind de SEM wel wat hoog uitvallen, want deze is in logit - units - dan dus al 2.5. Kan dit kloppen?
---

```{r psi, echo=FALSE, message=FALSE, warning=FALSE}

model_psi <- function(model){
  b_true <- model$beta_l$b #abilities uit dmodel object - b_true
  sim_data <- sim.raschtype(theta = b_true, b = model$fit$b) #simulate item data based on abilities from model and delta's
  colnames(sim_data) <- names(model$fit$b)
  sim_data$age <- model$beta_l$agedays / 365.25
  b_est <- ability(data = sim_data, items = names(model$fit$b), key = data.frame(item = names(model$fit$b), delta = model$fit$b), metric = "logit")$b #estimate new abilities based on simulated data - b_est
  
  #calculate parameters for PSI calculation
  var_b_est <- var(b_est, na.rm = TRUE)
  var_b_true <- var(b_true, na.rm = TRUE)
  var_error_est <- var(b_true - b_est , na.rm = TRUE)

  #psi calculation
  psi <- (var_b_est - var_error_est) / var_b_est
  var_error_true = (var_b_true / psi) - var_b_true
  psi <- var_b_true / (var_b_true + var_error_true)

   sem <- sqrt(var_error_est)
   return(round(c(psi = psi, sem = sem), 3))
}

model_psi(model)


```


