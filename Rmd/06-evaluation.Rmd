# Evaluation {#ch:evaluation}

> Author: Iris Eekhout

The nice properties of the Rasch model (c.f. Section
\@ref(sec:whyrasch)) only hold when the data and model agree. It is
therefore important to study and remove discrepancies between model
and data. This chapter explains several techniques that aid in the
evaluation of model fit.

* Item fit (\@ref(sec:itemfit))
* Person fit (\@ref(sec:personfit))
* Differential item functioning (\@ref(sec:dif))
* Item information (\@ref(sec:iteminformation))
* Reliability (\@ref(sec:reliability))

These topics address different aspects of the solution. In practice, we
have found that item fit is the most important concern.

## Item fit {#sec:itemfit}

The philosophy of the Rasch model is different from conventional
statistical modeling. It is not the task of the Rasch model to account
for the data. Rather it is the task of the data to fit the Rasch
model. We saw this distinction before in Section
\@ref(sec:adaptmodel).

The goal of model fit assessment is to explore and quantify how well
empirical data meet the requirements of the Rasch model. One way to
gauge model fit is to compare the observed probability of passing an
item to the fitted item response curve for endorsing the item.

The fitted item response curve for each item $i$ is modeled as: 

$$P_{ni}= \frac{exp( \beta_{n} - \delta_{i})}{1+exp(\beta_{n}-\delta_{i})}, $$ 

where $\beta_n$ is the ability of person $n$ and $\delta_i$ is the
difficulty of item $i$. It is thus natural to compare the observed and
modeled probabilities at various levels of ability.

### Well-fitting item response curves

```{r smoccmodel6, dependson = "smoccmodel", cache = FALSE}
data <- knitr::load_cache("smoccmodel", "data")
items <- knitr::load_cache("smoccmodel", "items")
model <- knitr::load_cache("smoccmodel", "model")
```

```{r ploticc, results = 'hide', fig.keep = 'all', fig.height = 7, warning = FALSE, fig.cap = '(ref:ploticc)'}
p1 <- dmetric::plot_p_d_item(data = data, model = model, items = "ddifmd005")[[1]]
p2 <- dmetric::plot_p_d_item(data = data, model = model, items = "ddigmm060")[[1]]
gridExtra::grid.arrange(p1, p2)
```

(ref:ploticc) Empirical and fitted item response curves for two
milestones from the DDI (SMOCC data).

Figure \@ref(fig:ploticc) shows the item characteristics curves of two
DDI milestones. The orange line represents the empirical probability
at different ability levels. The dashed line represents the estimated
item response curve according to the Rasch model. The empirical and
estimated curves are close together, so both items fit the model very
well.

### Item response curves showing severe underfit

These examples of item response curves fit the model nicely. There are
however many cases where things are less bright. 

```{r smoccfit4, warning = FALSE}
set.seed(12345)
plus_items <- paste0("hypgmd00", 1:5)
varlist <- list(adm = c("cohort", "subjid", "agedays"), 
                items = c(items, plus_items),
                cov = NULL)
data2 <- as.data.frame(data)
data2 <- data2 %>% 
  mutate(hypgmd001 = as.integer(runif(n = nrow(data2), 0, 2)), # flat
         rand   = runif(nrow(data2), 0 , 1),
         hypgmd002 = ifelse(agedays  > 65 & rand > 0.2, 1, 0), # flat2
         hypgmd002 = ifelse(agedays <= 65 & rand < 0.2, 1, hypgmd002), # flat2
         hypgmd003 = ifelse(agedays > 65, 0, 1), # gutman age
         hypgmd004 = ifelse(model$beta_l$b < 2, 0, 1), # gutman d
         hypgmd005 = ifelse(model$beta_l$b > 2 & runif(nrow(data2), 0, 1) > 0.1, 1, 0) # gutman d_error
         ) %>% 
  dplyr::select(-rand)
b_fixed <- model$fit$b
model2 <- dmetric::fit_dmodel(varlist = varlist, data = data2, 
                              name = "57_0_plus", b_fixed = b_fixed)
```


```{r plothyp1, results = 'hide', fig.keep = 'all', fig.height = 11, warning = FALSE, fig.cap = '(ref:plothyp1)', message = FALSE}
p1 <- dmetric::plot_p_d_item(data = data2, model = model2, items = "hypgmd001")[[1]]
p2 <- dmetric::plot_p_d_item(data = data2, model = model2, items = "hypgmd002")[[1]]
p3 <- dmetric::plot_p_d_item(data = data2, model = model2, items = "hypgmd003")[[1]]
gridExtra::grid.arrange(p1, p2, p3)
```

(ref:plothyp1) Three simulated items that illustrate various forms of item misfit.

Figure \@ref(fig:plothyp1) shows three forms of severe underfit. The
probability to pass item `hypgmd001` is almost constant across
ability, so retaining this item essentially only adds to the noise.
Item `hypgmd002` converges to an asymptote around 80 percent and has a
severe dip in the middle. The dip is caused by the strong relation to
age. Item `hypgmd003` appears to have the wrong coding. In addition,
the spike-like behavior in the middle is often seen when two or more
different items are erroneously given the same code.

Removing of items like these could substantially improve model fit.

### Item response curves showing overfit

```{r plothyp2, results = 'hide', fig.keep = 'all', fig.height = 7, warning = FALSE, fig.cap = '(ref:plothyp2)', message = FALSE}
p1 <- dmetric::plot_p_d_item(data = data2, model = model2, items = "hypgmd004")[[1]]
p2 <- dmetric::plot_p_d_item(data = data2, model = model2, items = "hypgmd005")[[1]]
gridExtra::grid.arrange(p1, p2)
```

(ref:plothyp2) Two simulated items that illustrate item overfit.

Figure \@ref(fig:plothyp2) shows two forms of overfit. The curve of item
`hypgmd004` is much steeper than the modeled curve. Thus, just this
one item is exceptionally well-suited to distinguish children with a
$D$-score below 50 from those with a $D$-score above 50. Note that the
item isn't sensitive anywhere else on the scale. In general, having
items like these is good news, because they allow us to increase the
reliability of the instrument. One should make certain though, that
FAIL and PASS scores are all measured (not imputed) values.

Multiple perfect items could hint to a violation of the *local
independence assumption* (c.f. Section \@ref(sec:measurementmodel)).
Developmental milestones sometimes have combinations of responses that
are impossible. For example, one cannot walk without being able to
stand, so the inconsistent combination (stand: FAIL, walk: PASS) will
not be observed. This leads to more consistent responses that would be
expected by chance alone. In principle, one could combine the two such
items into one three-category items, which effectively set the
probability of inconsistent combinations to zero.

Item `hypgmd005` is also steep, but has an asymptote around 80
percent. This causes discrepancies between the empirical and modeled
curves around the middle of the probability scale. In general, such
items could be removed if a sufficient number of better items are
available.

### Item infit and outfit

Item fit can quantified by item *infit* and *outfit*. Both are
aggregates of the model residuals. The observed response $x_{ni}$ of
person $n$ on item $i$ can be $0$ or $1$. The expected response
$P_{ni}$ is modeled as

$$P_{ni} = \frac{exp(\hat\beta_{n} - \hat\delta_{i})}{1 + exp(\hat\beta_{n} - \hat\delta_{i})}$$, 

where $\hat\beta_{n}$ is the estimated ability of person $n$, and
where $\hat\delta_{i}$ is the estimated difficulty of item $i$. The
standardized residual $z_{ni}$ is the difference between the observed
response $x_{ni}$ and the expected response $P_{ni}$, divided by the
expected binomial standard deviation:

$$z_{ni} = \frac{x_{ni}-P_{ni}}{\sqrt{W_{ni}}}$$, 

where the expected response variance $W_{ni}$ is calculated as 

$$W_{ni} = P_{ni}(1-P_{ni})$$. 

*Item infit* is the sum of the squared residuals divided by the sum of
the expected response variances $W_{ni}$.

$$\mathrm{Infit} = \frac{\sum_{n}^N (x_{ni}-P_{ni})^2}{\sum_n^N W_{ni}}$$

*Item outfit* is calculated as the average (over $N$ persons) of the
squared standardized residual

$$\mathrm{Outfit} = \frac{\sum_{n}^N z_{ni}^2}{N}$$

The expected value of both infit and outfit is equal to 1.0. The
interpretation is as follows:

* If infit and outfit are 1.0, then the item perfectly fit the Rasch
model, as in Figure \@ref(fig:ploticc);

* If infit and outfit > 1.0, then the item has underfit. The amount of
under fit is quantified by infit and outfit, as in \@ref(fig:plothyp1);

* If infit and outfit < 1.0, then the item fits the model better than
expected (overfit). The amount of over fit is quantified by infit and
outfit, as in \@ref(fig:plothyp2).

Infit is more sensitive to discrepancies in the middle of the
probability scale, whereas outfit is the better measure for
discrepancies at probabilities close to 0 or 1. Lack of fit is
generally easier to spot at the extremes. The two measures are highly
correlated. Infit is generally considered more important.

Values near 1.0 are desirable. There are no cut-and-dried [cutoff
values](https://www.rasch.org/rmt/rmt83b.htm) for infit and outfit. In
general, we want to remove underfitting items, e.g., item with infits
or outfits higher than, say, 1.3. Overfitting items with values lower
than 1.0 are not harmful. Preserving these items may actually help to
increase reliability of the scale. The cut-off chosen also depends on
the number of available items. When there are many items to choose
from, we could use a stricter criterion, say infit and outfit < 1.0 to
select only the absolutely best items.

### Infit and outfit in the DDI {#sec:fitddi}

```{r fitplot, fig.cap = '(ref:fitplot)', fig.height = 4}
oldpar <- par(mfrow = c(1, 2))
hist(model$item_lfit$infit, breaks = 20, col = "grey", 
     border = FALSE, xlim = c(0, 1.2), ylim = c(0, 12),
     main = "", xlab = "Item infit", ylab = "Number of milestones")
hist(model$item_lfit$outfit, breaks = 20, col = "grey", 
     border = FALSE, xlim = c(0, 1.2), ylim = c(0, 12), 
     main = "", xlab = "Item outfit", ylab = "Number of milestones")
par(oldpar)
```

(ref:fitplot) Frequency distribution of infit (left) and outfit
(right) of 57 milestones from the DDI (SMOCC data).

Figure \@ref(fig:fitplot) displays the histogram of the 57 milestones
from the DDI (c.f. Section \@ref(sec:ddi)). Most infits are within the
the range 0.6 - 1.1, thus indicating excellent fit. The two milestones
with extremely low infits are `ddigmd052` and `ddigmd053`. These two
items actually screen for paralysis for newborns, so the data contain
hardly any fails on these milestones. The outfit statistics also
indicate good fit.

## Person fit {#sec:personfit}

The person fit quantifies the extent to which the responses of a given
person conform to the Rasch model expectation. The Rasch model expects
that a more able person has a greater probability to pass an item than
a less able person. Fit analysis evaluates the extend to which this is
true. 

### Person infit and outfit

In parallel to item fit, we can calculate *person infit* and *person
outfit*. Both statistics evaluate how well the responses of the
persons are consistent with the model. The outfit statistic is heavily
influenced by outlying responses that do not fit the expected pattern.
Outfit is high, for example, when the person fails easy items but
passes difficult items. The infit is the information weighted fit and
is more sensitive to inlaying, on-target, unexpected responses.

Similar to item fit, person fit is also calculated from the residuals,
but aggregated in a different way. Person infit is calculated as

$$\mathrm{Infit} = \frac{\sum_{n}^L (x_{ni}-P_{ni})^2}{\sum_n^L W_{ni}}$$

and person outfit is

$$\mathrm{Outfit} = \frac{\sum_{n}^L z_{ni}^2}{L}$$

As a threshold for person fit < 3.0 can be used to clean out persons
with implausible response patterns.

### Person infit and outfit in the DDI

```{r personfitplot, fig.cap = '(ref:personfitplot)', fig.height = 4}
oldpar <- par(mfrow = c(1, 2))
hist(model$person_lfit$infit, breaks = c(seq(0, 1000, 0.25)), col = "grey", 
     border = FALSE, xlim = c(0, 5), freq = TRUE, ylim = c(0, 7000),
     main = "", xlab = "Person infit", ylab = "Number of measurements")
hist(model$person_lfit$outfit, breaks = c(seq(0, 1000, 0.25)), col = "grey", 
     border = FALSE, xlim = c(0, 5), freq = TRUE, ylim = c(0, 7000),
     main = "", xlab = "Person outfit", ylab = "Number of measurements")
par(oldpar)
```

(ref:personfitplot) Frequency distribution of person infit (left) and
person outfit (right) for 16538 measurements of the DDI (SMOCC data).

Figure \@ref(fig:personfitplot) displays the frequency distribution of
person infit and person outfit 16538 measurement of the DDI in the
SMOCC data. The large majority of the values falls below 3.0. For
infit, only 43 out of 16538 measurements (0.3 percent) is above 3.0.
For outfit, there are 446 out of 16538 measurement (2.7 percent) above
3.0. Expect the solution to improve after deleting these measurements.

## Differential item functioning (DIF) {#sec:dif}

### Relevance of DIF for cross-cultural equivalence

An important assumption in the Rasch model is that a given item has
the same difficulty in different subgroups of respondents. Climbing
stairs is an example where this assumption is suspect. The exposure to
stairs, and hence the opportunity for a child to practice, varies
across different cultures. It could thus be that two children with the
same ability but from different cultures have different success
probabilities for climbing stairs. When these probabilities
systematically vary between subgroup, we say there is *Differential
Item Functioning*, or *DIF* [@holland1983]. DIF is undesirable
since it can make the instrument culturally biased.

### How to detect DIF?

@zumbo1999 provided a clear definition of DIF:

> DIF occurs when examinees from different groups show differing
> probabilities of success on (or endorsing) the item after matching on
> the underlying ability that the item is intended to measure.

There are various ways to detect DIF. Here we will model the
probability of endorsing an item by logistic regression using the
observed item responses as outcome, and with ability, the grouping
variable, and the ability-grouping interaction as predictors. When the
latter two terms explain the residual variance of the item scores
after adjusting for ability, the item presents DIF for that group. DIF
can be visually inspected by plotting the curves for the subgroups
separately.

The are two forms of DIF: 

* *Uniform DIF*: The item response curve differ between groups in
location, but are parallel;

* *Non-uniform DIF*: The item response curve differ between groups in
location, in slope and possibly in other characteristics.

These forms correspond to, respectively, the main effect of group and
the ability-group interaction in the logistic regression model.

### Examples of DIF

```{r plotdif1, results = 'hide', fig.keep = 'all', fig.height = 7, warning = FALSE, fig.cap = '(ref:plotdif1)', message = FALSE}
p1 <- dmetric::plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddigmd063")[[1]]
p2 <- dmetric::plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddifmd011")[[1]]
gridExtra::grid.arrange(p1, p2)
```

(ref:plotdif1) Two milestones from the DDI with similar item response curves for boys and girls. There is no DIF for sex.

Figure \@ref(fig:plotdif1) shows an example comparing boys and girls.
For both milestones, the item response curves are similar for boys and
girls, so we see no evidence of DIF here.

```{r plotdif2, results = 'hide', fig.keep = 'all', fig.height = 7, warning = FALSE, fig.cap = '(ref:plotdif2)', message = FALSE}
p1 <- dmetric::plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddifmm019")[[1]]
p2 <- dmetric::plot_dif(data = data, model = model, dif = "sex", metric = "logit", item = "ddigmm064")[[1]]
gridExtra::grid.arrange(p1, p2)
```

(ref:plotdif2) Two milestones from the DDI with different item
response curves for boys and girls. There is evidence for uniform DIF.

Figure \@ref(fig:plotdif2) displays two milestones with DIF between
boys and girls. Milestone `ddifmm019` ("Takes off shoes and socks")
is easier for girls by about 0.86 logits (= the difference in ability
at the intersection of 50 percent). Conversely, milestone `ddigmm064`
("Crawls forward, abdomen on the floor") is easier for boys by about
0.84 logits. These are the largest differences found for sex in the
DDI. Both are primarily uniform DIF.

In practice, having milestones with opposite forms of DIF in the same
instrument will cancel out one another, so one need not be overly
concerned in that case. However, we should be careful when the
instrument consists of milestones that all have DIF in the same
direction.

## Item information {#sec:iteminformation}

### Item information at a given ability

Items are generally sensitive to only a part of the ability scale.
Item information is a psychometric measure that quantifies how
informative the item is at different levels of ability. Item
information can be plotted as a curve for each item.

```{r iic, message=FALSE, warning=FALSE, fig.height = 4, fig.cap = '(ref:iic)'}
items <- c("ddigmm060", "ddicmm039")
#item information curve
## bereken item info by difficulty
 info <- function(beta, delta) {(exp(beta-delta)/(1+exp(beta-delta)) * (1-exp(beta-delta)/(1+exp(beta-delta))))}
 delta <- model$fit$b[items[1]]
 betar <- c(-10,2)
 beta = seq(betar[1], betar[2], length = 200)
 iteminfo1 <- data.frame(ability=beta,I=info(beta,delta))

 delta <- model$fit$b[items[2]]
 betar <- c(-2,10)
 beta = seq(betar[1], betar[2], length = 200)
 iteminfo2 <- data.frame(ability=beta,I=info(beta,delta))

 iteminfo1$item <- paste(items[1], get_labels(items[1]))
 iteminfo2$item <- paste(items[2], get_labels(items[2]))
 df <- rbind(iteminfo1,iteminfo2)

 ggplot(df, aes(ability, I, group = item, color = item)) +
   geom_line() + 
   xlab("Ability (logit)") +
   ylab("Item information") +
   theme(legend.position = "bottom") +
   scale_x_continuous(breaks = seq(-10, 10, 2)) +
   guides(color=guide_legend(title = NULL))
```

(ref:iic) The item information curve for two milestones from the DDI. 

Figure \@ref(fig:iic) displays the item information curves for two
milestones from the DDI. Note that the amount of information
for the item is maximal around the item difficulty. In the Rasch
model, the information curves of different items are just translations
along the ability scale.

The formula to obtain the item information is the first derivative of
the item response curve and can be written as follows:

$$I(\delta)=P_i(\delta)(1-P_i(\delta))$$ 
where $P_i(\delta)$ is the conditional probability of endorsing an
item. For example for milestone `ddicmm039` ("Says three words") the $\delta_i$
equals $4.06$. The probability of endorsing this item for a person
with an ability level of $2$ standard deviation above the mean is

$$P_{ni}= \frac{\mathrm{exp}(2 - 4.06)}{1+\mathrm{exp}(2-4.06)} = 0.113$$

So for an ability level of $2$ standard deviations above the mean
$\beta_n = 2$, milestone `ddicmm039` has an information of

$$I(\delta)=0.113(1-0.113) = 0.1$$ 


### Item information at a given age

In practice, it is often interesting to express the item information
against age. By doing so, one can identify at what ages the item
provides the most information.

```{r iia, message=FALSE,  results = 'hide', fig.keep = 'all', warning=FALSE,  fig.height = 4, , fig.cap = '(ref:iia)'}
# model in logit
refs <- dmetric::calculate_references(model = model, metric = "logit", 
                                      trace = FALSE)
refs$month <- refs$age

#item information curve
## bereken item info by difficulty
 info_age <- function(betas, p = 50, reference) {
 pd <- matrix(betas, nrow = length(betas), ncol = length(p)) +
    matrix(1 * qlogis(p / 100),
           nrow = length(betas), ncol = length(p), byrow = TRUE)
  pa <- approx(x = reference$mu, y = reference$month, xout = as.vector(pd))$y
  pa <- matrix(pa, ncol = length(p))
  pda <- data.frame(round(pd, 2), round(pa, 2))
  names(pda) <- c("delta", paste0("A", p))
  pda
 }
  

 iteminfo1$age <-info_age(betas = iteminfo1$ability, reference = refs)$A50
 iteminfo2$age <-info_age(betas = iteminfo2$ability, reference = refs)$A50

 df <- rbind(iteminfo1,iteminfo2)
 
 ggplot(df, aes(age,I, group=item, color=item)) + 
   geom_line() + 
   xlab("Age (months)") + 
   ylab("Item information") +
   theme(legend.position = "bottom") +
   scale_x_continuous(breaks = seq(0, 60, 10)) +
   guides(color=guide_legend(title = NULL))
```

(ref:iia) Information information of Figure \@ref(fig:iic) plotted against age.

Figure \@ref(fig:iia) shows that the sensitive age ranges differ
considerably between items. Milestone `ddigmm060` is sensitive at
between ages 10--20 months, where `ddicmm039` is sensitive between
ages 27--47, a range that is twice as broad. Note also that the
information curves are no longer symmetric.

The item information by age curve helps to determine at what ages the
item can best be administered. The item will be most informative, when
administered at the age at which 50% of the children will pass the
milestone. Administering the item closely around that age provide the
most efficient measurement of ability. When space is at premium (e.g.
as in population surveys) using a well-chosen set of age-sensitive
milestones will help in reducing the total number of milestones that
needs to be measured.

In other contexts, milestones may be used as a screening instrument to
identify developmental delay. In that case, it is more efficient to
administer items that are very easy for the age, e.g. milestones on
which, say, 90% of the children will pass.

## Reliability {#sec:reliability}

Reliability is defined as the proportion of variance of the person
estimates (i.e. abilities) relative to the total variance, including
errors. More specifically, the reliability $R$ of a test is written as

$$R \equiv \frac{\sigma_{\beta}^2}{\sigma_{\beta}^2 + \sigma_{e}^2},$$ 

where $\sigma_{\beta}^2$ is the variance of true ability and
$\sigma_{e}^2$ is the error variance. 

Cronbach's $\alpha$ is a popular estimate of the lower bound
of the reliability of a test. In the Rasch model, we can estimate 
reliability by the [ratio](http://www.rummlab.com.au/rmrelidx2030)

$$\hat{R} = \frac{\hat\sigma_{\hat\beta}^2 - \hat\sigma_{\hat e}^2}{\hat\sigma_{\hat\beta}^2}.$$

For a given model, we can calculate $\hat\sigma_{\hat\beta}^2$
directly as the sampling variance of the estimated abilities, but
getting an estimate for $\hat\sigma_{\hat e}^2$ is more difficult. We
use the modeled abilities and modeled difficulties to generate a
hypothetical data set of the same size and same missing data pattern,
and re-estimate the abilities from the simulated data. Then
$\hat\sigma_{\hat e}^2$ is computable as variance of the difference
between the modeled abilities and the re-estimated abilities.

```{r psi, message=FALSE, warning=FALSE}

model_psi <- function(model, data){
  b_true <- model$beta_l$b #abilities uit dmodel object - b_true
  sim_data <- sirt::sim.raschtype(theta = b_true, b = model$fit$b) #simulate item data based on abilities from model and delta's
  items <- names(model$fit$b)
  colnames(sim_data) <- items
  
  # apply same missing data as real data
  data3 <- data[, items]
  sim_data[is.na(data3)] <- NA

  sim_data$age <- model$beta_l$agedays / 365.25
  b_est <- dscore::ability(data = sim_data, items = names(model$fit$b), 
                           key = data.frame(item = names(model$fit$b), 
                                            delta = model$fit$b), 
                           metric = "logit",
                           qp = seq(-25, 25, 0.25))$b #estimate new abilities based on simulated data - b_est
  
  #calculate parameters for PSI calculation
  var_b_est <- var(b_est, na.rm = TRUE)
  var_b_true <- var(b_true, na.rm = TRUE)
  var_error_est <- var(b_true - b_est , na.rm = TRUE)

  # psi calculation
  r1 <- (var_b_est - var_error_est) / var_b_est
  var_error_true = (var_b_true / r1) - var_b_true
  r2 <- var_b_true / (var_b_true + var_error_true)

   sem <- sqrt(var_error_est)
   return(round(c(r = r1, sem = sem, resid = b_true - b_est), 3))
}

results <- model_psi(model, data2)
```

```{r histres, eval = FALSE}
hist(result$resid, breaks = seq(-12, 2, 0.25), 
     col = "grey", border = FALSE, main = "", xlim = c(-8, 2),
     xlab = "Ability (estimated) - Ability (`true`) (logit)")
```

The estimated variance of the modeled abilities is
$\hat\sigma_{\hat\beta}^2 = 76.6$, and the variance of the difference
between modeled and re-estimated abilities is equal to
$\hat\sigma_{\hat e}^2 = 1.74$. The corresponding *standard error of
measurement (sem)* is $\hat\sigma_{\hat e} = 1.32$ logits.

The estimated reliability in the SMOCC data can now be calculated as
$(76.6-1.74)/76.6 = 0.977$. This number can be interpreted in the same
way as Cronbach's $\alpha$. The reliability is very high here because
of the large variation in $D$-scores. In practice, the *sem* is perhaps
a more relevant measure of precision.
