```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

# New model to quantify child development {#ch2:newmodel}

Chapter \@ref(ch:history) introduces the age-based, score-based and
unit-based approaches to measurement of child development. This
chapter introduce the basic concepts for the new unit-based approach.

## The latent variable, and its role in measurement

In this booklet we define measurement as 
*the process of locating children and items on a line.* The line can
represents a latent variable, a continuous construct that defines the
different poles of the concept that we want to measure. In our case,
the concept is *child development*. Other constructs, like *height*,
*temperature* or *happiness* would be equally valid constructs. In
practice, the position of the items on the line will be fixed when the
measurement instrument is created. Application of the instrument
involves placing children onto the line given their scores on one or
more of the items, which is the more usual activity that we describe
as "measurement". Realize however that both steps are needed.

A latent variable ranges from low to high, and has a constant unit.
Thus, interpretation of the same difference is identical across the
entire scale. A latent variable cannot be measured directly. It is a
theoretical construct. However, we may be able to actually measure
variables (test items) that are in some way related to the latent
variable. For example, we may have the child's scores on a set of
tasks, such as "stacking two blocks". The way in which the measured
variables are related to the latent variables is called the
*measurement model*.

Some measurement models attempt to explain systematic patterns in the
observed data through their relation to a single underlying variable.
For example, we may find that the observed data are highly correlated,
but that this correlation disappears within groups of children that have
the same value on the latent variable. In that case, we have absorbed
the common information in the variables into a single score that
captures the relevant information.

Probabilistic measurement models express the probability of passing an
item as a function of the difference between the person's ability and
the item's difficulty. A person with low ability will almost surely
fail a difficult item, whereas a highly able person will almost surely
pass an easy item. If we know the score on one or more items, and the
difficulties of these items, we may calculate the position of the
person on the latent variable.

### Criteria for invariant measurement

Measurement is the process of assigning numbers by rules to a set of
persons or objects. This definition is a little liberal. In order to
be useful the process of assigning numbers must be bound by some
imperatives. In this booklet, we strive to achieve *invariant
measurement*, a very strict form of measurements that is subject to
the following requirements (@ENGELHARD2013, p. 14):

1. *Item-invariant measurement of persons*: The measurement of persons
must be independent of the particular items that happen to be used for
the measuring.

2. *Non-crossing persons response functions*: A more able person must
always have a better chance of success on an item that a less able
person.

3. *Person-invariant calibration of test items*: The calibration of
the items must be independent of the particular persons used for
calibration.

4. *Non-crossing item response functions*: Any person must have a
better chance of success on an easy item than on a more difficult
item.

5. *Unidimensionality*: Items and persons must be simultaneously
located on a single underlying latent variable.

A statistical model that conforms to these five requirements is said
to be an *ideal-type model*. There are only very few ideal-type
models. An ideal-type model isn't changed to fit the data, but the
data (items and persons) are selected to fit the ideal-type model.

The need for the five requirements for invariance is not universally
shared. In particular, a substantial group of statisticians questions
the need for requirements 2 and 4, especially the words "always" and
"any", and may opt for more flexible models that do not strictly
adhere to the invariance conditions 2 and 4. There are also more
flexible models that assume a multi-dimensional instead of a
unidimensional latent variable. Some investigators question all
requirements, and would rather not be restricted and place a high
value to a good fit to all data.

### Item response functions

Let $\beta_n$ be the true (but unknown) developmental score of a child
at the time of measurement. The more generic term is the child's
*ability*. The *item response function* describes the probability of
passing an item as a function of ability. In general, we expect
that this probability increases with ability (requirement 2). Note
that this formulation differs from that given in Section
\@ref(agebased), where the probability was said to increase in age,
not ability. We will address this crucial difference in more detail in
Section \@ref(comparisonagebased).

```{r irfplot, echo = FALSE, warning = FALSE, fig.asp = 0.5, fig.cap = '(ref:irfplot)'}
pkg <- c("ggplot2", "RColorBrewer")
loaded <- sapply(pkg, require, character.only = TRUE, 
                 warn.conflicts = FALSE, quietly = TRUE)
options(knitr.kable.NA = "?")

x <- seq(-5, 5, 0.25)
items <- data.frame(
  Items = c(rep(LETTERS[1:5], times = c(4, 2, length(x), length(x), length(x)))),
  beta = c(-5, 1, 1, +5,  -5,  +5, x, x, x),
  pass = c( 0, 0, 1,  1, 0.3, 0.3,
           plogis(x),
           plogis(0.6 * (x + 1)),
           smooth.spline(x = x, y = plogis(c(x[1:15], rep(x[16], 10), x[26:41])), df = 8)$y))
ggplot(items, aes(x = beta, y = 100 * pass, group = Items, colour = Items)) +
  geom_line(lwd = 0.75) + 
  scale_x_continuous("Child's ability (logits)", breaks = -5:5) + 
  scale_y_continuous("% pass", breaks = seq(0, 100, 20), limits = c(0, 100)) +
  scale_colour_brewer(palette = "Set1", labels = c("A: Step", "B: Constant", "C: Logistic", "D: 2PL", "E: Monotone")) + 
  theme_light() + 
  theme(legend.position = c(0.05, 0.95), legend.justification = c(0, 1))
```

(ref:irfplot) Item response functions for five hypothetical items,
each demonstrating a positive relation between ability and probability
to pass.

Ability and passing probability can be related in many ways. Figure
\@ref(fig:irfplot) illustrates the item response function of five
hypothetical test items A-E. Let us consider each of these function.

- A: Item A has step function, so all children with an ability < 1
logit fail, and all children with ability > 1 pass. This is the ideal
item for discriminating children that with abilities below and above 1
logit. The step function is widely used in sports. In a high-jumping
tournament, the bar is set very high in order to be able to
discriminate the best athletes. Note that the step function can only
test whether ability is below or above a specific level. It does
discriminate abilities at other levels.

- B: For item B, the probability of passing is constant at 30 percent.
This 30 percent is not related to ability. This item does not measure
ability, and only adds to the noise. It is a bad item for measurement.

- C: Item C has a smoothly increasing logistic function. If we know
ability $\beta$, the probability of passing can be calculate by the
standard logistic function $f(\beta) = e^\beta/(1+e^\beta)$. This
item can discriminate best around the point at which the probability
of passing is 0.5. The level is called the difficulty. Unlike the 
step function, this item can also discriminate abilities in the
neighborhood of the difficulty. 

- D: Item D is also a smoothly increasing logistic function, but it
has an extra parameter that allows it to vary its slope (or
discrimination). The extra parameter can be the item steeper (more
discriminatory) than the green curve, in the limit approaching a step
curve. It can also become shallower (less discriminatory) than the
green curve (as plotted here), in the limit approaching a constant
curve. Thus, item D is more general that items A, B or C.

- E: Item E is even more general in the sense that it need not be
logistic, but can be any monotonically increasing function. As
plotted, the item is insensitive to abilities between -1 and 0 logits,
and more sensitive to abilities of 0 to 2 logits.

These are just some examples of how the relation between the child's
ability and passing probability could look. In practice, the curves
need not start at 0 percent or end at 100 percent. They could also be
U-shaped, of have other non-monotonic forms. See @COOMBS1964 for a
thorough overview of such models. Here we restrict to increasing
functions so as to conform to invariance requirement 2.

### Person response functions

We now switch the roles of persons and items, and construct a function
that tells us how likely it is that a single person can pass an item, 
or more commonly, a set of items. 

<!-- Let $\delta_i$ be the true (but unknown) *difficulty* of an item. The -->
<!-- *person response function* is the probability that a single person passes  -->
<!-- each of a set items of varying difficulties.  -->

Let us continue with items A, C and D from Figure \@ref(fig:irfplot), and 
calculate the response function for three children, respectively with 
abilities $\beta_1 = -2$, $\beta_2 = 0$ and $\beta_3 = 2$.

```{r prfplot, echo = FALSE, warning = FALSE, fig.asp = 0.5, fig.cap = '(ref:prfplot)'}
x <- c(seq(-5.00, -2.25, 0.25), -2.001, -2, -1.999, 
       seq(-1.75, -0.25, 0.25), -0.001,  0,  0.001,
       seq( 0.25,  1.75, 0.25),  1.999,  2,  2.001,
       seq( 2.25,  5.00, 0.25))
A1 <- c(rep(1, 23), 0.5, rep(0, 23))
C1 <- 1 - plogis(x)
D1 <- 1 - plogis(0.6 * (x + 1))
ACD1 <- (A1 + C1 + D1) / 3

A2 <- c(rep(1, 33), 0.5, rep(0, 13))
C2 <- 1 - plogis(x - 2)
D2 <- 1 - plogis(0.6 * (x - 1))
ACD2 <- (A2 + C2 + D2) / 3

A3 <- c(rep(1, 13), 0.5, rep(0, 33))
C3 <- 1 - plogis(x + 3)
D3 <- 1 - plogis(0.6 * (x + 4))
ACD3 <- (A3 + C3 + D3) / 3

data <- data.frame(delta = x,
                   person = rep(c("-2", "0", "+2"), each = 47),
                   pass = c(ACD3, ACD1, ACD2))
ggplot(data, aes(x = delta, y = 100 * pass, group = person, colour = person)) +
  geom_line(lwd = 0.75) + 
  scale_x_continuous("Item difficulty (logits)", breaks = -5:5) + 
  scale_y_continuous("% pass", breaks = seq(0, 100, 20), limits = c(0, 100)) +
  scale_colour_brewer(name = "Person ability", palette = "Set1", limits = c("-2", "0", "+2")) + 
  theme_light() + 
  theme(legend.position = c(0.8, 0.6), legend.justification = c(0, 0)) +
  annotate("text", x = -4.5, y = 15, label = "Easy items", fontface = "bold") + 
  annotate("text", x =  4.5, y = 15, label = "Hard items", fontface = "bold")
```

(ref:prfplot) Person response functions for three children with abilities 
-2, 0 and +2, using a small test of items A, C and D.

Figure \@ref{fig:prfplot} presents the person response functions from
three persons with abilities of -2, 0 and +2 logits. The functions are
calculated as the average of response probabilities on items A, C and
D. Thus, on average, we expect that child 1 will pass an easy item of
difficulty -3 in about 60 percent of the time, whereas for an
intermediate item of difficulty of -1 the passing probility would be
10 percent. For child 3, with a higher ability, these probabilities
are quite different: 97% and 90%. The substantial drop in the middle
of the curve is caused by the step function of item A. 

Requirement 2 states that the success probability should be higher for
persons with a higher ability level. This is indeed the case here, but
there are also item response functions for which requirement 2 does
not hold.

### What is a test?

A set of items forms a test. The primary reason for administering a
test rather than a separate item is to improve the precision of the
measurement. The scores on the set of items (rather than the score on
a single item) can be combined into an overall score. Of course,
combining scores can only make sense if these items measure the same
underlying construct.

In principle, the five items A-E in Figure \@ref(fig:irfplot) could
form a test. This would not be an ideal test though, since this test
would violate requirement 4. We know that the difficulty of item A is
equal to +1 logits, and the difficulty of item C is 0 logits, so item
C is easier than item A. Requirement 4 says that persons should have a
higher chance of passing C. This is true indeed for persons having an
ability lower than +1 logits. However, for a person with ability +2
logits, the probability of passing A is higher than passing C, hence
requirement 4 does not hold for any test that holds both items A and 
C.


### Interval scale

The important property is that $\beta_1$, $\beta_2$ and $\beta_3$ are
all on the same scale, and hence can be compared in a sensible way. In
particular, suppose that $\beta_2 - \beta_1 = \beta_3 - \beta_2$, we
might say that the difference between ability $\beta_1$ and $\beta_2$
is the same the difference between abilities $\beta_2 - \beta_3$. This 
is the essential property of an interval scale.



### Family of item response theory models

*Ideal-type models*:

1. Guttman scalogram model (@GUTTMAN1950)

2. Rasch model (@RASCH1960) and extensions (@WRIGHT1982, @ANDRICH1988)

3. Mokken scaling model (@MOKKEN1971).

The Guttman and Mokken model yield an ordinal latent scale, while the 
Rasch model yields an interval scale (with a constant unit).


## Comparison to classic age-based approach {#comparisonagebased}

EXAMPLES OF AGE-BASED MEASUREMENT

Let the $Y_j$ contain the scores on the $j$-th developmental item, where $j = 1, \dots, p$. 
For the moment, let us assume that all items are binary, so that 
$Y_j = 0$ indicates a fail, and $Y_j = 1$ indicates a pass. 
Suppose we are interested in studing the relationship between age, denoted by 
variable $X$, and development as measured through items $Y_1, \dots, Y_p$.

The joint distribution $P(Y_1, \dots, Y_p, X)$ contains all relevant 
information for this problem. Age-based measurement is an attempt to model 
this joint distribution by breaking it down into $p$ conditional distributions:
$$
P(Y_1 | X)\\
\vdots\\
P(Y_p | X)\\
$$

each of which describes how the probability of passing changes with age. 
For example, @DRACHLER2007 formulated the model as a logistic regression 
model on the logarithm of age:

$$ 
\ln \frac{P(Y_j=1)}{P(Y_j=0)} = \alpha_j + \beta_j\ln X
$$

For this model, the age at which 50\% of the children pass item $j$ 
can be calculated as to $\exp(-\alpha_j/\beta_j)$. Since older children are 
expected to be more mature, this parameter is typically 
interpreted as the difficulty of the item. The slope parameter 
$\beta_j$ indicates how rapidly the probability of passing 
rises with age. Items with high $\beta_j$ have a great power to discriminate 
developmental status well within a small age range. It is straightforward
to calculate any other age-percentiles, for example, the ages 
at which probabilities 10\% or 90\% of the sample pass the item.
Table~2 in @DRACHLER2007 reports these statistics for the Denver 
developmental test.

While the idea of breaking up the test into separate items
is productive, age-based measurement has severe weaknesses. 

1. The concepts of age and development are not separated, so we 
cannot interpret the difficulty of an item without age. 
It would be more convenient, and conceptually clearer, if 
item difficulty could be a free-standing concept.

2. The models are fitted separately for each $Y_j$. Nothing 
is specified how the items relate to each other, so in principle, 
any variable that changes with age (e.g. child had first birthday) could 
qualify as a developmental item. It would be better of the 
procedure would start from a model about how the items should 
relate to each other. 

3. The intercept and slope estimates are dependent on 
the sample. A sample consisting of children of low ability
would give different estimates than a sample of children with
high ability. Calibration of test items is not person-invariant. 
It would be preferable if the difficulty estimates
were independent of the sample used to estimate them. 

4. Because of the introduction of slopes in the logistic model
a more able person may have a lower success probability for some
items than a less able person. It would be preferable if 
more able persons have a better succes rate on all items.
